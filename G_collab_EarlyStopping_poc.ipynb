{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic EarlyStopping class"
      ],
      "metadata": {
        "id": "G2AE4_pJnJjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    \"\"\"\n",
        "        basically we have best loss which is lowest val loss we seen during training,\n",
        "        \n",
        "        The min_delta parameter represents the minimum change in the\n",
        "        monitored quantity to qualify as an improvement.\n",
        "\n",
        "        To better understand min_delta relation with current val_loss and best_loss Suppose:\n",
        "\n",
        "                self.best_loss is 0.5\n",
        "                val_loss is 0.4\n",
        "                self.min_delta is 0.1\n",
        "            Then, we have:\n",
        "                self.best_loss - val_loss = 0.5 - 0.4 = 0.1\n",
        "                0.1 >= 0.1\n",
        "\n",
        "            Since 0.1 is greater than or equal to 0.1, the condition is satisfied, indicating an improvement.\n",
        "\n",
        "            Now let's consider another scenario:\n",
        "            Suppose:\n",
        "                self.best_loss is 0.5\n",
        "                val_loss is 0.55\n",
        "                self.min_delta is 0.1\n",
        "            Then, we have:\n",
        "                self.best_loss - val_loss = 0.5 - 0.55 = -0.05\n",
        "                -0.05 >= 0.1\n",
        "            Since -0.05 is not greater than or equal to 0.1, the condition is not satisfied, indicating no improvement.\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "id": "6azRRuTSpu4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_model = None\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.status = \"\"\n",
        "\n",
        "    def __call__(self, model, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "        elif self.best_loss - val_loss >= self.min_delta:\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
        "            if self.counter >= self.patience:\n",
        "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
        "                if self.restore_best_weights:\n",
        "                    model.load_state_dict(self.best_model)\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "k2sZ5Xnkc6o9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple neural network architecture for MNIST\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input size is 28x28 for MNIST images\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output size is 10 for 10 classes (digits 0-9)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input images\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the SimpleNN model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define transformations and load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize EarlyStopping object\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):  # Assume 20 epochs for demonstration\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{200}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Check early stopping criteria\n",
        "    print(early_stopping.status)\n",
        "    if early_stopping(model, val_loss):\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, retrieve the best model weights\n",
        "best_model = model  # Assuming early_stopping restores the best model automatically\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am2JX8p3c6gz",
        "outputId": "23f215a6-dbf4-4a24-d363-3f6777457686"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Validation Loss: 0.2762, Accuracy: 92.17%\n",
            "\n",
            "Epoch 2/200, Validation Loss: 0.2262, Accuracy: 93.43%\n",
            "\n",
            "Epoch 3/200, Validation Loss: 0.1789, Accuracy: 94.53%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 4/200, Validation Loss: 0.1628, Accuracy: 95.18%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 5/200, Validation Loss: 0.1366, Accuracy: 95.91%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 6/200, Validation Loss: 0.1208, Accuracy: 96.24%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 7/200, Validation Loss: 0.1120, Accuracy: 96.56%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 8/200, Validation Loss: 0.1005, Accuracy: 96.86%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 9/200, Validation Loss: 0.1076, Accuracy: 96.49%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 10/200, Validation Loss: 0.0944, Accuracy: 96.97%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 11/200, Validation Loss: 0.0959, Accuracy: 97.07%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 12/200, Validation Loss: 0.0850, Accuracy: 97.40%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 13/200, Validation Loss: 0.0846, Accuracy: 97.41%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 14/200, Validation Loss: 0.0864, Accuracy: 97.31%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 15/200, Validation Loss: 0.0815, Accuracy: 97.41%\n",
            "No improvement in the last 2 epochs\n",
            "Epoch 16/200, Validation Loss: 0.0821, Accuracy: 97.34%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 17/200, Validation Loss: 0.0804, Accuracy: 97.48%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 18/200, Validation Loss: 0.0803, Accuracy: 97.62%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 19/200, Validation Loss: 0.0887, Accuracy: 97.20%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 20/200, Validation Loss: 0.0779, Accuracy: 97.61%\n",
            "No improvement in the last 2 epochs\n",
            "Epoch 21/200, Validation Loss: 0.0807, Accuracy: 97.52%\n",
            "Improvement found, counter reset to 0\n",
            "Epoch 22/200, Validation Loss: 0.0858, Accuracy: 97.51%\n",
            "No improvement in the last 1 epochs\n",
            "Epoch 23/200, Validation Loss: 0.0838, Accuracy: 97.65%\n",
            "No improvement in the last 2 epochs\n",
            "Epoch 24/200, Validation Loss: 0.0855, Accuracy: 97.61%\n",
            "No improvement in the last 3 epochs\n",
            "Epoch 25/200, Validation Loss: 0.0786, Accuracy: 97.80%\n",
            "No improvement in the last 4 epochs\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets modify it a little bit to add more print statements"
      ],
      "metadata": {
        "id": "_vyTLFRBmCd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_model = None\n",
        "        self.best_loss = None\n",
        "        self.best_accuracy = None  # New attribute to track best accuracy\n",
        "        self.counter = 0\n",
        "        self.status = \"\"\n",
        "\n",
        "    def __call__(self, model, val_loss, val_accuracy):\n",
        "        if self.best_loss is None or val_loss < self.best_loss:  # Update if val_loss is better\n",
        "            self.best_loss = val_loss\n",
        "            self.best_accuracy = val_accuracy  # Update best accuracy along with best loss\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.counter = 0\n",
        "            self.status = f\"Improvement found, counter reset to {self.counter}. \" \\\n",
        "                          # f\"Best Loss: {self.best_loss:.4f}, Best Accuracy: {self.best_accuracy:.2f}%\"\n",
        "\n",
        "        elif self.best_loss - val_loss >= self.min_delta:\n",
        "            self.best_model = copy.deepcopy(model.state_dict())\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.status = f\"Improvement found, counter reset to {self.counter}. \" \\\n",
        "                          # f\"Best Loss: {self.best_loss:.4f}, Best Accuracy: {self.best_accuracy:.2f}%\"\n",
        "\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            self.status = f\"No improvement in the last {self.counter} epochs. \" \\\n",
        "                          # f\"Best Loss: {self.best_loss:.4f}, Best Accuracy: {self.best_accuracy:.2f}%\"\n",
        "            if self.counter >= self.patience:\n",
        "                self.status = f\"Early stopping triggered after {self.counter} epochs. \" \\\n",
        "                              f\"Best Loss: {self.best_loss:.4f}, Best Accuracy: {self.best_accuracy:.2f}%\"\n",
        "                if self.restore_best_weights:\n",
        "                    model.load_state_dict(self.best_model)\n",
        "                return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "y3z94jH6lMD3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple neural network architecture for MNIST\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input size is 28x28 for MNIST images\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output size is 10 for 10 classes (digits 0-9)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input images\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the SimpleNN model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define transformations and load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize EarlyStopping object\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(200):  # Assume 20 epochs for demonstration\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{200}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Check early stopping criteria\n",
        "    # print(early_stopping.status)\n",
        "    if early_stopping(model, val_loss,accuracy):\n",
        "        print(early_stopping.status)\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Optionally, retrieve the best model weights\n",
        "best_model = model  # Assuming early_stopping restores the best model automatically\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAcVE84KlMAX",
        "outputId": "50d869dc-df9e-438e-9d45-2faf38999aab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Validation Loss: 0.3109, Accuracy: 90.69%\n",
            "Epoch 2/200, Validation Loss: 0.2487, Accuracy: 92.82%\n",
            "Epoch 3/200, Validation Loss: 0.1969, Accuracy: 94.38%\n",
            "Epoch 4/200, Validation Loss: 0.1647, Accuracy: 95.00%\n",
            "Epoch 5/200, Validation Loss: 0.1381, Accuracy: 95.64%\n",
            "Epoch 6/200, Validation Loss: 0.1356, Accuracy: 95.87%\n",
            "Epoch 7/200, Validation Loss: 0.1156, Accuracy: 96.28%\n",
            "Epoch 8/200, Validation Loss: 0.1016, Accuracy: 96.82%\n",
            "Epoch 9/200, Validation Loss: 0.1042, Accuracy: 96.65%\n",
            "Epoch 10/200, Validation Loss: 0.0943, Accuracy: 97.07%\n",
            "Epoch 11/200, Validation Loss: 0.0892, Accuracy: 97.22%\n",
            "Epoch 12/200, Validation Loss: 0.1093, Accuracy: 96.69%\n",
            "Epoch 13/200, Validation Loss: 0.0894, Accuracy: 97.12%\n",
            "Epoch 14/200, Validation Loss: 0.0837, Accuracy: 97.36%\n",
            "Epoch 15/200, Validation Loss: 0.0897, Accuracy: 97.26%\n",
            "Epoch 16/200, Validation Loss: 0.0947, Accuracy: 97.17%\n",
            "Epoch 17/200, Validation Loss: 0.0833, Accuracy: 97.44%\n",
            "Epoch 18/200, Validation Loss: 0.0795, Accuracy: 97.50%\n",
            "Epoch 19/200, Validation Loss: 0.0868, Accuracy: 97.38%\n",
            "Epoch 20/200, Validation Loss: 0.0879, Accuracy: 97.46%\n",
            "Epoch 21/200, Validation Loss: 0.0846, Accuracy: 97.46%\n",
            "Epoch 22/200, Validation Loss: 0.0806, Accuracy: 97.65%\n",
            "Epoch 23/200, Validation Loss: 0.0787, Accuracy: 97.41%\n",
            "Epoch 24/200, Validation Loss: 0.0906, Accuracy: 97.44%\n",
            "Epoch 25/200, Validation Loss: 0.0941, Accuracy: 97.23%\n",
            "Epoch 26/200, Validation Loss: 0.0854, Accuracy: 97.63%\n",
            "Epoch 27/200, Validation Loss: 0.0796, Accuracy: 97.59%\n",
            "Epoch 28/200, Validation Loss: 0.0814, Accuracy: 97.54%\n",
            "Early stopping triggered after 5 epochs. Best Loss: 0.0787, Best Accuracy: 97.41%\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another flavour of Earlystopping"
      ],
      "metadata": {
        "id": "i4b5fAhAnB4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gwqZo7Q-XIBB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True, path=\"best_model.pth\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.path = path\n",
        "\n",
        "        self.best_model = None\n",
        "        self.best_loss = None\n",
        "        self.best_acc = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def save_checkpoint(self, model, val_acc):\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "                    'val_acc': val_acc}, self.path)\n",
        "\n",
        "    def should_stop(self, model, val_loss, val_acc):\n",
        "        if self.best_loss is None:\n",
        "            self._initialize_best_loss(val_loss, val_acc)\n",
        "        elif self._is_improvement(val_loss):\n",
        "            self._update_best_loss(model, val_loss, val_acc)\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self._no_improvement()\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self._trigger_early_stopping()\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _initialize_best_loss(self, val_loss, val_acc):\n",
        "        self.best_loss = val_loss\n",
        "        self.best_acc = val_acc\n",
        "        self.best_model = copy.deepcopy(model.state_dict())\n",
        "        print(\"Initialized best loss and accuracy.\")\n",
        "\n",
        "    def _is_improvement(self, val_loss):\n",
        "        improvement = self.best_loss - val_loss >= self.min_delta\n",
        "        if improvement:\n",
        "            print(\"Improved validation loss.\")\n",
        "        return improvement\n",
        "\n",
        "    def _update_best_loss(self, model, val_loss, val_acc):\n",
        "        self.best_loss = val_loss\n",
        "        self.best_acc = val_acc\n",
        "        self.best_model = copy.deepcopy(model.state_dict())\n",
        "        print(\"Updated best loss and accuracy.\")\n",
        "\n",
        "    def _no_improvement(self):\n",
        "        print(\"No improvement in validation loss.\")\n",
        "\n",
        "    def _trigger_early_stopping(self):\n",
        "        print(f\"Early stopping triggered after {self.counter} epochs with no loss improvement.\")\n",
        "        if self.restore_best_weights:\n",
        "            print(\"Restoring best weights.\")\n",
        "            model.load_state_dict(self.best_model)\n",
        "            self.save_checkpoint(model, self.best_acc)\n",
        "            print(f\"Accuracy of the best model: {self.best_acc:.2f}%\")\n",
        "        self.early_stop = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_data, val_data = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize EarlyStopping object\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001, restore_best_weights=True, path=\"best_model.pth\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):  # Let's say we're training for 20 epochs\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            output = model(data)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    # Check if early stopping criteria are met\n",
        "    if early_stopping.should_stop(model, val_loss, val_acc):  # Pass the 'model' argument as well\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV7hhs6SXJrb",
        "outputId": "2a992017-0eba-40c6-8dad-496d45ac00cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 0.2552, Validation Accuracy: 92.40%\n",
            "Initialized best loss and accuracy.\n",
            "Epoch 2, Validation Loss: 0.1828, Validation Accuracy: 94.69%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 3, Validation Loss: 0.1642, Validation Accuracy: 95.15%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 4, Validation Loss: 0.1271, Validation Accuracy: 96.03%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 5, Validation Loss: 0.1240, Validation Accuracy: 96.33%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 6, Validation Loss: 0.1087, Validation Accuracy: 96.67%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 7, Validation Loss: 0.1130, Validation Accuracy: 96.63%\n",
            "No improvement in validation loss.\n",
            "Epoch 8, Validation Loss: 0.1080, Validation Accuracy: 96.74%\n",
            "No improvement in validation loss.\n",
            "Epoch 9, Validation Loss: 0.1124, Validation Accuracy: 96.70%\n",
            "No improvement in validation loss.\n",
            "Epoch 10, Validation Loss: 0.0966, Validation Accuracy: 97.26%\n",
            "Improved validation loss.\n",
            "Updated best loss and accuracy.\n",
            "Epoch 11, Validation Loss: 0.1066, Validation Accuracy: 96.78%\n",
            "No improvement in validation loss.\n",
            "Epoch 12, Validation Loss: 0.1052, Validation Accuracy: 97.03%\n",
            "No improvement in validation loss.\n",
            "Epoch 13, Validation Loss: 0.1189, Validation Accuracy: 96.69%\n",
            "No improvement in validation loss.\n",
            "Epoch 14, Validation Loss: 0.1108, Validation Accuracy: 96.92%\n",
            "No improvement in validation loss.\n",
            "Epoch 15, Validation Loss: 0.1098, Validation Accuracy: 97.06%\n",
            "No improvement in validation loss.\n",
            "Early stopping triggered after 5 epochs with no loss improvement.\n",
            "Restoring best weights.\n",
            "Accuracy of the best model: 97.26%\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nS0_iBv-X2T0"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}